Ameera Kawash  0:00  
Get started, and then I'll explain a little bit more about what we are building and designing.

Hulma  0:05  
Yeah, sure. Thank you. So my name is Hulma and I did my bachelor's in software engineering. I graduated in 2020, and since then, I've been in this data science, ml sort of domain. I work for around a couple of years, a little more than a couple of years in the industry, before I went for my master's, my master's has been solely focused on NLP. It was an Erasmus masters, and we were basically doing a lot of linguistics, computational linguistics, as well as some traditional just deep learning and machine learning stuff. And my thesis focused on using NLP, basically, sort of an agentic pipeline, or the current Lang chain, sort of agentic, but more like a bash agentic sort of file. And yeah, so it was basically generating prediction to start instructions using Lama, lama 3.3 to 70 million version. And yeah, for the past year, I've also been working with this startup, PLT hub, where I also have applied some AI techniques with some sort of data, basically shifting their platform from GCP to bet and airflow to abuse costs. So yeah, that's a big introduction, but yeah, feel free to ask any questions.

Ameera Kawash  1:19  
Fantastic, great. Just to give you a bit of overview about what we're building, because I don't think I went into it much into the in the last call, but initially, we're building a bilingual meeting intelligence and like knowledge production suite with a focus initially on Arabic and English. So there's a lot of work with ASR models, with whisper models. And then we're building as well, like knowledge infrastructure on the back end of that. And you know where we're trying to get to that? Because now, at least with a lot of dialects in Arabic, the ASR models, open sources are performing really poorly. They don't understand a lot of phonetics. They don't understand idiom cultural understanding and so on. So we've been working with Prava car on building this agentic framework, and, you know, for for improving, initially, ASR models, dialectical ASR models. So that's kind of like the the most immediate thing. And we're actually very close to completing, like, a very function, functioning MVP, like, probably, you know, two, three weeks away. And then we're going to use that as a kind of foundation to continue to build the knowledge of production work and so on. So that's kind of the immediate, immediate task. There's this, we do have another, like app or platform that we are also going to be raising funds and so on to build, which will be more multimodal, but that will build on some of these infrastructures so but it will be working with, you know, machine and vision models, and working to do bias prompts and outputs and so on. So that's kind of like where we are, and Prabhakar has been working on this for some months now, and we're looking to kind of expand, tighten up production eyes as well. Some of what, some of what we've been doing on the the ML and the agentic side of things, and that's kind of where you will be coming in in this, in the life cycle of where we are. I think it's an exciting time because we are going to start to to kind of better launch it with our, you know, partners and people from the region, universities, cultural orgs and so on, to start to actually, you know, use, use this for dialectical, and yeah, for meeting and meeting intelligence initially. So if you want to ask about that, I guess you're welcome to do so now. Or we can also, you know, just dig a bit deeper into your experience and into your kind of how what would you prefer? Do you want to ask anything specifically?

Hulma  4:09  
I think more about the context of, is this for building some sort of application? Is it more like for research? What is the ultimate goal? For example, is this is solving some sort of problem, yeah. How does it work? Yeah.

Ameera Kawash  4:23  
I mean, the immediate goal is that it's going to be used for meeting intelligence. So it's a bilingual like order replacement, right? Because, right. Sorry, do you want to speak

Prabhakar Ray  4:34  
just one minute? Did someone

Ameera Kawash  4:39  
just so the you know, there is a need. You know, Arabic ASR models perform really, really poorly. You know, the error rates are terrible. There isn't, there isn't quite a tool out there for that region. And you know, beyond that, we want to start, really, to move into capturing, you know, knowledge, knowledge from the region, cultural knowledge, sorry, an understanding. So I guess the you the the next, the next phase or the next market will be like. More about using this tool also for, for Yeah, for knowledge production, for research, oral histories, for media transcription and so on. But, yeah, it is a commercial product.

Hulma  5:27  
Yeah, sounds good. I think I don't have any questions at the moment, so feel free to ask. Okay, very cool.

Ameera Kawash  5:35  
Yeah, someone gonna repeat, because Walid will also watch this video later, I think, or rather read the transcript. So yeah. So I just just, it'd be interesting to know a little bit more about how you got involved with with AI, like, what were, what were the things that drew you to it? Where are you now? Where do you see yourself now in the journey? And, like, what are your motivations, like, moving forward,

Hulma  5:56  
yeah. So if you go really back, I absolutely loved mathematics since I was a kid, and I was also very much into the computer science domain. So during my Bachelor's, I wasn't very sure what I will do, but I came across this machine learning courses. I took a few of them, and it was a very math heavy so I really, really liked it. That's how I went into AI, and that's why I my. Basically, my final year project for bachelor's was a computer vision model, basic model for classifying recyclable and non recyclable categories in waste items. So yeah, that's how I went into data science in industry as well. And then in my final six months before the Masters, I think I started exploring a bit of NLP, because I wanted to take up some more tasks. They were around chat bots and stuff. So I also realized I lack a lot in the NLP side. So that's why I went for the Masters as well. And yeah, it was a great learning experience. And right now, like when I've graduated, I would love to be in a similar domain. I do love to code. So I would love to develop something around NLP, some sort of application like you guys are working for energetic setup for this problem. So, yeah, I just, I think right now I'm willing to explore whatever we can I can do with my masters and the skill set. Yeah, by the

Ameera Kawash  7:20  
way, provokar, if you want to jump in at any point, like you have full license too, it's, it's up to you.

Prabhakar Ray  7:30  
I think one question from my side would be, you know, just not a question, basically. But it would be great to know more about, you know, your master thesis, where I think you said you use NLP for your for healthcare?

Hulma  7:43  
Yes, yes. So yeah. So I will explain this now. So basically, we're trying to generate, we were trying to generate patient discharge instructions using LLM. So there's a whole data set of discharge summaries that was present from a New York Hospital that is open source. So we took that, and there's already some research going on. There was a shared task recently where the top teams performing were using basically fine tuned models of even the large language models, or like smaller versions of those. So yeah, but it was all mostly focused on fine tuning, and what the previous research was doing was some eval the evaluation metrics. They also did some human evaluation using some clinicians, but the automatic evaluation metrics were not we found out that they were not really performing very well for this specific task. So there was a need for enhanced faithfulness and readability, because it's a patient facing document, and also completeness, like we can't really miss anything very critical. So we identified, since we were also collaborating with the shared task holders, we had the access to previous submissions. So we identified that there was some very critical errors highly ranked by the current evaluation metrics, because they were mostly focused on blue rouge, and sometimes also align score, which does not capture factuality in sorry, in the healthcare domain that well, because it's a complex one. So yeah, we identified that there were some certain issues with that. And what we did was we came up with this whole prompting approach. We use llama 70 billion version, and we sort of created an agentic setup where we generated the start instructions, evaluated them using llms, and then that feedback, verbal feedback, was then passed again to the LLM to improve on that. So we were short on time and resources. So we didn't really do a whole iteration of when we reach our final basically, good score, but we did one iteration, and the basically future research can do that, and our prompting approach was in this way. So we basically tried to employ the reason and act the react based framework. So the reasoning step here was to generate a plan to generate discharge instructions, and this was through the LLM so we give it the input data, we give it some solved examples, and we ask it to generate what sort of questions do you think the discharge instruction should answer, or what steps it should take? And then once we have this plan, we then generate the discharge instructions to another LLM call. And then when we have the first version, we do an LLM evaluation for faithfulness. So we do a sentence by sentence grounding. We basically ask the model to ground this sentence in the original input data and whether this is faithful or not So, and also generate some explanation why I think so. So it's a verbal feedback that we can then pass back to the LLM. And there were two other metrics as well, readability score. We were just using fresh Kincaid grade level, readily available in Python. And also we were using a completeness metric, scoring out of five, by giving it some basically grading scale onto what to look for. So these three metrics, when combine it to a verbal feedback, we pass them back to the LLM, and it revised on the previous version. And we did see that there was, if you think, if we assume that these metrics are really working, which we think were was the case, then they really, really improved after just one iteration, like from 40% to 70% and for these metrics, if we run them on previous submissions, they were not really doing that well. Although they were very highly, they were performing better than us on automatic, automated evaluation metrics. So yeah, this was the whole thing that we contributed. And yeah, do you have any questions? Yeah, no, that

Prabhakar Ray  11:41  
sounds interesting. It's, it's really interesting. I think, I think a lot of parallels actually, because the kind of task which we are trying to do here is, is also where you don't have a very clearly defined metrics. You know, you can use, as you said, you can use blue, or you can use rook, or you can use WR or CR, but they don't really work very well. You need, you need a better metric. So I guess from your, if I understand the description that you gave correctly, there was a part where you brainstormed, and you come up, you came up with your new metrics, like readability and these sort of metrics. And you also define what actually readability means, you know. So you said, like, I think you said that you did some prompt things. So basically, I am assuming that you define, you are defining the meaning of each metric and how LLM should be able to, you know, evaluate, should evaluate a new transcript or new text based on what instructions it has been given. It's kind of an LLM as a judge model,

Hulma  12:51  
yes, I would say for completeness part, it was the case entirely. We just gave some instructions on how to grade, and it was grading, but for faithfulness, I think by asking it to ground something in the input data, we were already inherently telling us it what we want faithfulness to be like, right? And I think that is factually correct as well. But for readability, the scores were just flash. Can get paid level. It wasn't really coming from the LLM, but basically improving the breathability level, which was the second part was done through the LLM, and it really did improve the fish skin guide level as well. So yeah, the metrics were very unclear for us as well. There was a lot of time that was spent on what can we even consider as evaluation metric? What can we add to it? So, yeah, it really did work.

Prabhakar Ray  13:43  
Okay, no, that's, that's very interesting. So, I mean, if you have to list maybe three challenges that you the like, three challenges you faced in this project, what would that?

Hulma  13:56  
I think the first biggest challenge was compute. So there was limited compute on the university clusters, and there was a Q system as well, which was just quite a mess back then when I started. It's quite better now. So compute was a big issue. Other than that, I think what else I think coming up with the whole pipeline from start to end, it wasn't challenging, but was a lot of work as well. So like coming up with evaluation and then reiterating based on the evaluation, so it was a lot of work, and we could consider it as a challenge. And maybe the third thing could be, I think

Hulma  14:49  
there was also, like a maybe, like, I searched a lot, but I could not really find a lot of metrics evolving over time as well. Like maximum, you can have, like, Alliance for perpetuality, but yeah, this was a challenge, like finding good metrics, and that's why we ended up creating some pseudo metrics ourselves.

Prabhakar Ray  15:13  
Yeah, yeah. And what was the size that you had for evaluation? I You mentioned fine tuning. Was it just fine from a fine tuning? Or you fine tune the model also.

Hulma  15:24  
So we did not do any fine tuning. It was previous research that were was doing fine tuning. So we were trying to do it through prompt, basically prompt chaining. So we did not really use the training data, except for passing three short samples and on validation data we did not use at all as well because we were short on the resources. So it was 10,000 discharge summaries that we tried to generate it for, like eventual experimentation. But initially we were doing it on a smaller sample of the training data to just have a good working pipeline, but it was around 10,000 and I think it's the total size of the data set is around 100,000 but we didn't really do it for the entire data.

Prabhakar Ray  16:06  
Yeah, no, no, that makes sense. I mean, let's say, when you're doing this iteration, maybe you would have picked maybe 100 or something, some examples to iterate and fine tune the problems. Yeah, I guess the data would be like you have, you have the original discharge summaries, and you have some parameters on, based on, based on which this discharge summary was produced. And you gave the LLM, the parameters, not the discharge summary. And then you compare the discharge summary that came out of the LLM, and the original discharge summary is that,

Hulma  16:41  
yes, there's something that I did not mention. So the entire discharge summary basically contains a lot of sections. So right now the focus was to just automate one part of it. So the input was basically all the other sections of the discharge summary, and this was just discharge instructions for the patient. This was the part that we were trying to automate. There are also other sections, like medicines. You can't really automate it now with the current state of the art. And then, yeah, so we were passing the input data as all the other sections, and only the discharge instruction was the cold.

Prabhakar Ray  17:20  
Okay, so that means, that means that it also had kind of summarizing also, or kind of using that as a knowledge bank.

Hulma  17:28  
So summarizing the other sections of the discharge summary, you mean, right? Yeah.

Prabhakar Ray  17:35  
So I'm, I think what I'm understanding is that there are other sections of the district summary and maybe one section, maybe it could be, you know, the the first section of summary section, or something like that, which just lists what, basically everything from this, from the other segments, basically, is that it or is using new knowledge based on in

Hulma  18:01  
the discharge instruction. Instructions, are we creating anything that was not a part of,

Prabhakar Ray  18:06  
yeah, where the LLM has to think on its own, not just rely on the knowledge that it has been given,

Hulma  18:12  
yeah, yeah. I think it definitely was going beyond summarization, because it could mention, like they were mentions of, okay, what, for example, medicine do you have to take, but what are the side effects of the medicine that the LLM had to come up with, it on itself and, like, if there's a certain sort of surgery or anything, then the LLM was coming up with, you know, what a sort of care instructions should be there for a person who has to go through surgery, yeah. So there was a lot of other things as well, but there was also a lot of part that was coming from other sections directly, yeah,

Prabhakar Ray  18:49  
okay, I think I might stop here. Ameera, you

Ameera Kawash  18:53  
can I was just kind of because you also say that you work with llms and rags as well on blog automation. So this in the medical evaluation the discharge were there, was there, were there rags as well, was it? Was it just and how, maybe you can tell us also a bit about your experience working with rags or knowledge, knowledge banks, yeah.

Hulma  19:17  
So there was a plan initially to have the entire data set, training data set as a rag and maybe bring up the most related input data and use that star instruction as, basically as part of the input to come up with this. But yeah, there was just not a lot of time. There was already a lot of moving parts, so we didn't explore that area. But yeah, for the blog automation part, I did work on this wasn't really agentic. I think that was a mistake that I put in my CV. I had a, not a great idea of what agentic is. So agent It was basically a prompt chaining pipeline I was trying to generate through three different two different rags and one SEO optimization part. So these tags were basically generating so for example, I want to generate a blog. We had this whole technical knowledge base that we scraped from the internet, and then we converted this into our database, the vector database. And for example, if I have the query, how to ensure data quality and data engineering, the most related top and blogs that I have in my rag, in my database, will come up, and then I will come up with one good, technically sound block. And the next part of it was having marketing knowledge. So for this, we just basically took a couple of marketing books and then converted it into chunks and then had them as a rag database. And the technical, technically sound block from the first part was passed along with this rack to modify it in from a marketing perspective. And then the third part was just basically grabbing some keywords from tools like Ahrefs, where you know what sort of volume is coming up for searches, and then putting those keywords into naturally into the blog that was done also through the so I was just using GPT API, not really any open source model for this. It's commercial task, right? So, yeah, do you have any questions?

Ameera Kawash  21:22  
Yeah, I guess I would like to know a little bit more about, you know, I mean, there we are also, you know, we're using this agentic frameworks improve foundation models. There's a lot of ambiguity, there's also evaluation, there's metrics, and then there's also working with linguists and transcribers and really trying to absorb and take their knowledge their feedback, you know, whether it's human in the feedback loops at the right time, or also to build up knowledge banks and etc. So there is this also very key part of you know, human in the feedback loops and and working with people who are experts in their domains in order to develop the right benchmarking or and also, you know, transfer some of their knowledge and expertise into the system. So have you had the experience, and what are your thoughts kind of like about, you know, human feedback loops? Have you worked with them and just, you know, your thoughts about, you know, the importance of that, let's say, in creating, you know, reliable systems.

Hulma  22:22  
Yeah, so I think there was a plan to initially do this for my thesis. We couldn't really get the help from any clinicians we did have. So there was another PhD student who was working with me on this task, and he had a doctor friend, so we were indirectly talking to him about some feedback on some of the generations. And then I think looking at it from my perspective, I cannot really come up with a good start instruction, even if I'm given the input. So there's really, really, there was really a need to have a human in the loop to identify what were the weaknesses over there. So he really helped with that. And if there was time, then we would have done as previous research did. We would have also gone for a clinician scoring system so that we knew what if it's really, really that helpful in a practical domain. And yeah, I think especially for cases where there's safety concern, for example, health care, we really, really need to have a human in the loop for now.

Prabhakar Ray  23:22  
Yeah, yeah. Just one question I had about this that you said we had, you know, multiple metrics, and a system which is scoring llms output on these metrics, and then you also had a system which took all these the scores, or, you know, took these scores and generated what should be changed in the output, the main output. So I understand that if you have multiple metrics, then you have multiple suggestions coming in. Then how did you, you know, how did you apply that to the final output?

Hulma  24:06  
Yeah, so this was just maybe naive, but very strangely, we just concatenated the feedback for this is the feedback for faithfulness. These are not basically grounded. This is the feedback for completeness. This is our readability score. And then in the in one single prompt. We were doing the whole revision for all three and surprisingly, it did work for, like, all the metrics.

Prabhakar Ray  24:29  
Yeah, okay. And did you also find, find that one metric is basically bleeding into the other, like, for example, one agent or a let's call an agent which is responsible for, let's say faithfulness, and another agent which is responsible for, let's say clinical accuracy. And both have their own instructions, and when they're outputting the feedback, the faithfulness agent is ending up suggest, suggesting things which clinical accuracy agents should be doing. Did you find these things where the agents are bleeding into each other?

Hulma  25:10  
So there's only completeness and factuality that was coming from the LLM and readability was not a part of it, but we this was one of our concerns, because when you take a discharge instruction and you try to make it more readable, you sometimes end up having less factuality. So these metrics are still interrelated, and it was a challenge to have both independently working well. But I think that is occurs. We can't really do it. We, like you, had to take care of both metrics and then come up at some like, you could really get a score of seven, which is a very good score for readability, but it will be a bunch of very simple terms of, you know, non factual. What really happened was very different. So this was a challenge again. I don't know if I answered your question.

Prabhakar Ray  25:57  
No, no, yeah, no, yeah. I mean, this is the challenge. That's why I thought that if you, if you face this kind of problem, it's bound to happen. But, yeah, I mean, there are things that we should be doing, and we could be doing to avoid these things, or at least have guardrails in place. So that's a good thing to have to know about. I think just one more questions I have, question I have, I'm just it's an interesting project. So I was thinking, you know, for 100 examples, if you're doing this kind of evaluation, for 100 examples, maybe we can manually look at those scores. You know, 100 is still a lot for a human, but maybe we can look at that score manually. But if you're doing, let's say for 1000 examples. Yeah, then, I'm not saying that you would have used some tools or something, but do you do, you know, what we can do to, you know, do these large scale experiments in a way that everything is logged so that we can look at that experiment? You know, basically everything is recorded, and we can go there, we can look at the experiments. We can see what version of prompts we were using before or these sort of things. Did you find any tool that were that was helpful in this kind of thing?

Hulma  27:13  
So there was a lot of iteration for our code. So we were using get and the prompt. Prompts were actually stored in such a way that we had agenda prompting. So we had just basically formats and every new prompt. There was this tool that the PhD student was working on with his friend, and we were trying to use that. It's a banks for prompt tool, and we were trying to use that in our experiments and for code, I think it was just a Git sort of, you know, keeping track of history. And I have worked before with some weights and biases, because it was a free version, so during my masters, that's a maximum we could do. And I think there's a tool called Neptune that I've also worked with briefly, but yeah, the most, most of the tracking for experiments and everything was done inside by basically storing different files and stuff like that.

Prabhakar Ray  28:10  
Yeah. Okay, no, yeah, good. I think just wanted to know, because you mentioned some of the tools, weights and biases is something which is there, from a long time, there are tools which are doing the same thing, but there are LLM specifics to help you more. But of course, fits and biases is something which is the foundation of it, and everything is built on top of it. So good, yeah? And then good to know, I think Ameera To want to ask anything else,

Ameera Kawash  28:38  
yeah, yeah. I do, um, I guess just a couple more questions. Just earlier, you said that, you know compute was an issue, and obviously, you know efficiency, you know, both energy wise, but also cost wise, is going to be a factor in this, in this project, and then it should be in all projects. So can you talk a little bit more about what, what did you do, like, How did you solve like issues around compute. Do you have any experience with, you know, inference, or kind of or other strategies for running things more efficiently and making sure that you don't like too many deployments of agents or too much sort of excessive, sort of record, record generation? Or just, yeah, if you could just speak to that a little bit more. Yeah.

Hulma  29:23  
So for hosting the model, I think it's already efficient when we were doing it with VLM. So it was a huge model, and that's how we were serving it. And then we were basically calling it the GPT transfer. So the GPT Functions in Python. So it was already efficient in that sense, but the most challenge was actually to even grab four GPUs, for example, not really a lot of time. So from my perspective, I think what I did was I added, rather than just storing the entire experiment after it's done, I added checks like after every 50 to 50, you just keep store, storing your outputs, so that you don't keep regenerating for them. And also, we were doing in batches. So sometimes 32 would work, sometimes 64 would work, depending on how much memory we had at that point. So yeah, I don't have a lot of experience with the tools that are, you know, host like hosting, llms, except for BLM. So yeah, does that answer your question? Yeah, sorry,

Prabhakar Ray  30:34  
I said that one last question, but one more question I have. So it's just to just to understand how you go about this. So I think you started with llama. But in many situations, we have seen that we also need to evaluate the LLM models also now. So would Is there a, is there a, you know, kind of recipe you follow to choose your LLM models to start with. Let's say there is a task and you want to, you know, compare, you know, how would you like start which LLM to start with? You know, that experiment or which set of llms You want to start with? Yeah.

Hulma  31:16  
So, since it was my master's thesis, there was, it was highly experimental, and we already started with a lot of models, and even the most facing the 8 billion version and after doing some experiments and evaluating them. But for this case, we only use the automatic evaluation metrics, which is, again, something we cannot really decouple So, yeah, we evaluated it on 102 sample size and use different models. We even currently, since we want to maybe publish this, we want to try a medical model as well. There's med gamma by Google Now. So if that works out, maybe we can just, you know, this is an experiment that we did initially, but there were no medical models we could use at that point that was suiting us.

Prabhakar Ray  32:05  
Yeah, yeah. Did you also, you know, consider the core architecture of the llms, you know, different Yeah? I mean, different architectures have different flair to it, and they're good at different kind of tasks. So did you also like look into Did you get a chance to explore this, which architecture would be good for a certain type of task,

Hulma  32:30  
not really for this, but I can't remember now, but I might have done some other project where this was a case, but for this one, I think we very initially just explored a bit of llama and some other models, and then only experimentally, by checking the evaluation metrics, we decided which. But that's a good point, I think, to focus on a model which is good for that, for that specific task, is, yeah, probably works better.

Prabhakar Ray  32:59  
No, no. I mean, that's, fine. Just, I mean, I know master's thesis, I've worked with a lot of master's students when I was doing my PhD. And is it three months? I think three

Hulma  33:11  
months. So mine was a very long one. I think I spent around one year and three months with this. We did a lot of work in

Prabhakar Ray  33:19  
this nice so you had some time. But I've seen people working on this, working on their masters projects, especially here in the UK. You have one year. Your thesis is just three, four months, and you do a lot of work in that period. Yeah, you explain a few things. You get you know about few things that you would like to explore later on, but, yeah, good to know. Thank you.

Ameera Kawash  33:46  
Yeah, another, I guess, challenge of this project, which Prabhakar can also speak to as well, is that, you know, you'll be collaborating with linguists and native speakers and so on to, you know, on the prompt writing on the evaluation were focused initially on Arabic dialects, particularly in the Levant. So Prabhakar worked with an LLM expert from the American University of Beirut who was specialized in Lebanese dialects. And then we have a group of transcribers. We bring on board, on linguists and so on. But obviously, you know, this comes with challenges, because you don't, I mean, I should, I should ask you a bit about your language background. I guess you know, how if you, if you, if you are multilingual, and so on. But also, how do you think you could cope with that as a challenge? I mean, one of the reasons we are also doing, you know, we're doing it this way is because we are hoping that there are frameworks that we can apply to low resource languages, and that, you know, there is a kind of pipeline for working with the linguists and the transcribers, you know, in a way that becomes repeatable. And hopefully, you know, we start to look at other languages as well, that related ones in particular. But yeah, so just it'd be great to hear a little bit about your experience with languages, if you have any with translation. And how do you think you could you would kind of meet this challenge. Yeah.

Hulma  35:17  
So I have, I am multilingual, I speak Urdu, I speak English, and I think I can't write, but I speak Hindi technically. And I also speak a little bit of German, so I understand this part. And there were some tasks during my masters, basically the entire masters, where we worked a lot on translation and summarization in languages that we don't don't really understand, and there was some focus on extinct languages, or close to extinct languages as well. So I do understand that, of course, when I looked at the task, for example, I couldn't really understand whether it is performing well or not. So for this reason, we do need someone who understands the language to come in and tell us basically, the weaknesses of our model, weaknesses of our outputs. So, yeah, I think with GPT now you can get a lot of help understanding languages faster. So you can ask it whether we are performing well or not with someone who actually knows the language is probably very important.

Ameera Kawash  36:19  
Yeah, absolutely. I mean, if, yeah, I mean, Prabhakar has been, you know, at the helm of that for some months, and I think, you know, has been navigating really well. But if there's anything you want to ask him specifically about that, you know, or if you want to say anything specific about that, public are easier,

Prabhakar Ray  36:37  
yeah, I mean, no, I think that's a good point to bring, bring up Ameera, I think, yeah, as also mentioned, it's, it's very important to have this kind of pipeline where you can get quick feedback, apart from the metrics, because metrics tell you, Okay, this is doing well or not, but an expert, native speaker of that language can tell you what's missing in the prompt, basically, what this line, whatever the output is, where exactly it's struggling. It's kind of, you're teaching a new language to someone. So the way you work, and then you figure out what, what, what is the weakness here, and what should I do strengthen this weakness, that's, that's, that's an interesting part of this, yeah, and it's very enjoyable.

Ameera Kawash  37:28  
It would be great. I mean, yeah, you speak a few languages, and you speak Urdu. And, I mean, you know, Urdu is another language that, you know, we would love to to work on. I mean, I'm not so as aware of how the like ASR outputs are, but you know, or do Swahili, you know, North African is performing extremely poorly. And you know, it's kind of creating, also some translation pairings that don't necessarily have to go through, like latent English, you know, and so on, you know, that's all quite an emergent field. So, yeah, it would be good, I guess, to maybe hear from you a bit about, like, also your, you know, have you had experience working, let's say, or, what is like, what is Urdu, AI been like for you? Like, in your experience, like, is there been a lot of, you know, what have you encountered, or in any, in any aspect, in any, any cultural aspect, like sort of issues around representational bias, issues around, you know, cultural understanding, cultural flattening, stereotyping, and just, you know, there's the kind of poor comprehension oftentimes that These dominant AI systems, which are very western centric kind of exhibits. So you could talk a bit about that your frustrations, or, you know, what might excite you about working on, really, on a project that is really focused on tackling some of these harms and misrepresentations? Yes.

Hulma  39:01  
So the current state of AI in Urdu is not really good, so we do have issues, just like you said, for example, if we try to translate an idiom in Urdu to English, it will actually literally translate it so it does not recognize the idioms and everything. And one thing that we also explored in some of our courses was biases related to gender, for example, or stereotypes as well. So for example, if you ask, since Urdu is not gender, I think it's a gendered language when you have different verbs or auxiliary verbs for one gender and not the other, yeah, yes, I think Arabic is like that as well. So there was also, we were working with German language as well at that point. So there were biases around when you say something, like the doctor is doing this or then it will turn out to be a male in the translation. And then if you do a nurse, for example, then it can turn out to be a female. So there are a lot of biases. And I think I came across very good research from some fellows as well, on some journalism biases again as well. And then, yeah, it is an important issue to talk about. And Urdu is not really, has not really been studied very well, I think abroad.

Ameera Kawash  40:17  
Yeah, great. I think one my last question early on, using them in the interview, we're just saying that you started off with machine vision, and that was and then you felt you didn't know enough about LMS, and then you and then you kind of went down that sort of rabbit hole. Are you still interested in machine vision? Is this something that you're still working on? Are you trying to kind of re engage with it. Or have you, you feel like you're too far down the path that you're on now? Or how are you kind of thinking about that?

Hulma  40:49  
I think after my masters, I got really interested in exploring more topics. So for example, Agent AI is not really core. NLP, it's more like a software development I would say. So yeah, it's really interesting, but computer vision, I haven't had the chance to work with for a very long time now. I would love to explore it, and I don't think it's too different. It is still different from NLP, but the modeling and everything, core concepts and everything still match. So yeah, my first machine learning course was focused around computer vision, so it would be great to work on that as well. But yeah, some sort of NLP and development maybe would be a nice, good mix. Yeah.

Prabhakar Ray  41:33  
Okay, so I think on that, I remember on that just comes, it just came to me that now we have multimodal models as well. They not really are llms, but share this in underlying architecture. So have you, have you worked with any of these multimodal really?

Hulma  41:57  
No, if you consider GPT, other than that, nothing, really, no, okay, no, no.

Prabhakar Ray  42:02  
Just, just, they're called visual language models, and they're, they're, like, very popular these days in robotics, where, you know, you can look at the scene and understand open. AI did a very famous in, you know, demo last year, couple of years back, where you can show the math problem and it can solve and can understand the all that. So, yeah, good. I mean, not a problem. It's just something that's just

Ameera Kawash  42:30  
also interested then, since we're just, yeah, have you What about like, blip and clip? Have you worked with them at all? Like, because that's another area that, you know, we are starting to build up is interesting because of the amount of bias and stuff and the prompting, and then also in the kind of, like encoding and decoding of, like vision models and stuff so, but yeah, anyway, just interest, interested to know. But that's great, fantastic. I think, I think I'm good on my end. Prabhakar, do you want to ask anything? Should we go into anything in a bit more technical detail, and also, yeah, you know we will, and you can absolutely ask us questions as well. If you want to know anything.

Prabhakar Ray  43:17  
I think nothing. From my side, I think it's good to know about your experiences with evaluation, because it's a very evaluation heavy project, we have to come up with a lot of different ways of robustly evaluating this system, whether it's synthetic data or LLM As a judge, you know different methods, basically to evaluate output, evaluate our agentic system. Right now is just in the future. It could be tense. I don't want to increase everyone go above any in one system, because it becomes very messy. But even with 10 agents, evaluating those 10 agents outputs together is something very challenging and something interesting to work on. So just to also, to let you know that is a very evaluation heavy whatever you're doing so, and it was very interesting to know about, you know, the metric that you the evaluation pipeline that developed in your So, yeah, I think I don't have any other questions,

Ameera Kawash  44:33  
it's almost there. If you have any questions, we're happy to, you know, we're here to, you know, to answer. But yeah. And also, yeah, I mean, Prabhakar will be staying on for a while, but, you know, we hope to have a, you know, period where sort of, he has more support as well while he's here, and then he'll be kind of in and out. So, yeah,

Hulma  44:56  
yeah, I don't think I have any questions, but yeah, it was really nice

Ameera Kawash  44:59  
chatting. Really nice chatting as well. And if you could, I know you mostly have your CV on LinkedIn, but if you could actually send me proper CV as well, I would really appreciate this. Yeah, perfect.

Hulma  45:09  
I'll just send this after all.

Ameera Kawash  45:11  
Okay, thank you so much, really. Thank you. Great chatting with you. Thank you. Thank you. Okay, soon. Bye. Take care. Bye, great. Yeah, yeah, what did you, what did you think?

Prabhakar Ray  45:28  
I mean, she's good, she's good, and she's she's a beginner, obviously, but I think she started. Yeah, it was a good first project to do on llms and and then I really like the song evaluation, you know, because that's something that's really important in in our case, and in any kind of systems we need to have to be able to focus on creating more metrics and have an evaluation, good evaluation pipeline. Yeah, yeah, yeah.

Ameera Kawash  46:02  
She's young. I mean, she's a recent graduate, you know, like, but I think, I mean, she's touched upon the basis, like she has the nascent parts for a lot of the things that we're working on, like, also working in a kind of, what is it like, a very ambiguous. There isn't. It's not like you're following a framework. You have to figure out how to construct, you know, metrics, and how to, you know, build agents and but, yeah, she's, she's, she's, you know, she's young, she's starting out, kind of thing.

Prabhakar Ray  46:36  
Yeah, that's not a problem. I think I'll be available anyway, so it'll be just, you know, a good support, and she knows she's, I think she would be better at evaluating, you know, because Urdu is non Arabic, I understand that, but some sense from that language. But, yeah, yeah, she's good. She's good,

Ameera Kawash  47:04  
yeah. I mean, an ideal scenario, like, she would be working with you for a couple of, like, whatever, a few weeks, you know. And then, you know, you've already, you know, you've put out such a great like, you know, you have the frameworks, the research, we really just polish them, yeah, the pipeline, and, you know, then in that time period, she either is kind of like growing and learning and really, you know, rising to the occasion. I think she's got enthusiasm. I think she's smart. I think, yeah,

Prabhakar Ray  47:35  
she's definitely smart. And she can think, she can think, I think from what I got, she can think about the main aspects of this whole pipeline. And whatever system that we have is already, you know, the foundation is already there. Now adding more agents is very easy, and the only thing remaining that we have to do is, you know, prompt, fine, yeah, knowledge or understanding where the agent is lacking. But the the main structure is very It

Ameera Kawash  48:04  
might also be, you know, it could either go a couple of ways. It could go that we take this framework and we start to adapt to, or do, I mean, it's a huge need there as well, if the model, and then we, and then she obviously has an advantage, and also the scripts. And, you know, there's, there will be, I think translation wise event over time or due to Arabic, will, I don't know. I mean, maybe I'm mind this conjecture, but I mean, that's one, one way it goes, we move some other languages. The other way it goes is that we start developing more agents around cultural understanding and sensitivity, so it becomes a bit more. Yeah, deep, or kind of culturally resonant, yeah. So, but I think Maheen.

